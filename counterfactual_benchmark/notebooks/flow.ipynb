{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d143269",
   "metadata": {},
   "source": [
    "# Counterfactual Benchmark Notebook\n",
    "- This notebook compacts what this repository does by showing an example for a GAN for Celeba Simple dataset\n",
    "- This notebook must be execute in-place inside the repo\n",
    "- Clone the repo and get a conda env with the correct packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f689e5",
   "metadata": {},
   "source": [
    "# Part 1: Training\n",
    "- Start by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700455f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from root to use local packages\n",
    "import os, sys\n",
    "os.chdir('..')\n",
    "from models.gans.celeba_gan import CelebaCondGAN\n",
    "from datasets.celeba.dataset import Celeba\n",
    "from models.classifiers.celeba_classifier import CelebaClassifier\n",
    "import torch\n",
    "import joblib\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from datasets.transforms import SelectParentAttributesTransform\n",
    "from models.utils import generate_checkpoint_callback, generate_early_stopping_callback, generate_ema_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89eed5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA RTX 3500 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90664d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "# from torchvision.datasets import CelebA\n",
    "# from torchvision.transforms import Resize, ToTensor, CenterCrop, Compose, ConvertImageDtype\n",
    "# import torch\n",
    "\n",
    "# transforms = Compose([CenterCrop(150), Resize((64, 64)), ToTensor(), ConvertImageDtype(dtype=torch.float32),])\n",
    "\n",
    "# data = CelebA(root=\"datasets\\celeba\\data\", split=\"train\", target_type=\"attr\", transform=transforms, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc6bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_class, attribute_size, config, transform=None, **kwargs):\n",
    "    data = data_class(data_dir=\"datasets\\celeba\\data\", attribute_size=attribute_size, transform=transform, split='train', **kwargs)\n",
    "\n",
    "    if data.has_valid_set:\n",
    "        train_set = data\n",
    "        val_set = data_class(data_dir=\"datasets\\celeba\\data\", attribute_size=attribute_size, transform=transform, split='valid', **kwargs)\n",
    "    else:\n",
    "        train_set, val_set = torch.utils.data.random_split(data, [config[\"train_val_split\"], 1 - config[\"train_val_split\"]])\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"batch_size_train\"], shuffle=True, num_workers=7)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=config[\"batch_size_val\"], shuffle=False, num_workers=7)\n",
    "    return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1099e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, config, data_class, graph_structure, attribute_size, checkpoint_dir, **kwargs):\n",
    "    transform = SelectParentAttributesTransform(\"image\", attribute_size, graph_structure)\n",
    "\n",
    "    train_data_loader, val_data_loader = get_dataloaders(data_class, attribute_size, config, transform, **kwargs)\n",
    "\n",
    "    monitor= \"fid\" if config['finetune'] == 0 else \"lpips\"\n",
    "    callbacks = [\n",
    "        generate_checkpoint_callback(gan.name, checkpoint_dir, monitor=monitor),\n",
    "        generate_early_stopping_callback(patience=config[\"patience\"], monitor=monitor)\n",
    "    ]\n",
    "\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "                      callbacks=callbacks,\n",
    "                      default_root_dir=checkpoint_dir, max_epochs=config[\"max_epochs\"])\n",
    "\n",
    "    trainer.fit(gan, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cls = {\n",
    "    \"attribute_size\": {\n",
    "        \"Smiling\": 1,\n",
    "        \"Eyeglasses\": 1\n",
    "    },\n",
    "\n",
    "    \"dataset\": \"celeba\",\n",
    "    \"ckpt_path\" : \"../methods/deepscm/checkpoints/celeba/simple/trained_classifiers\", #modified this line for the notebook\n",
    "    \"in_shape\" : [3, 64, 64] ,\n",
    "    \"patience\" : 10,\n",
    "    \"batch_size_train\" : 128,\n",
    "    \"batch_size_val\" : 128,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"max_epochs\" : 1000,\n",
    "    \"ema\": \"True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e352d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define causal graph (canibalised from config/../gan.json)\n",
    "causal_graph = {\n",
    "        \"Smiling\": [],\n",
    "        \"Eyeglasses\": [],\n",
    "        \"image\": [\"Smiling\", \"Eyeglasses\"],\n",
    "    }\n",
    "\n",
    "#define the models for each mechanism (only one for image here as the rest in the graph are roots)\n",
    "mechanism_models =  {\n",
    "        \"image\": {\n",
    "            \"model_type\": \"gan\",\n",
    "            \"model_class\": \"CelebaCondGAN\",\n",
    "            \"module\": \"models.gans\",\n",
    "            \"params\": {\n",
    "                \"n_chan_enc\": [3, 64, 128, 256, 256, 512, 512],\n",
    "                \"n_chan_gen\": [512, 512, 256, 256, 128, 64, 3],\n",
    "                \"latent_dim\": 512,\n",
    "                \"num_continuous\": 2,\n",
    "                \"d_updates_per_g_update\": 1,\n",
    "                \"gradient_clip_val\": 0.5,\n",
    "                \"finetune\": 1,\n",
    "                \"pretrained_path\": \"\",\n",
    "                \"lr\": 1e-4,\n",
    "                \"batch_size_train\": 128,\n",
    "                \"batch_size_val\": 128,\n",
    "                \"patience\": 10,\n",
    "                \"max_epochs\": 1000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "attribute_size = {\n",
    "        \"Smiling\": 1,\n",
    "        \"Eyeglasses\": 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5948d8f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'CelebaCondGAN' has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mCelebaCondGAN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'CelebaCondGAN' has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "CelebaCondGAN.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100fee9",
   "metadata": {},
   "source": [
    "### Train model:\n",
    "- Train the GAN model for Celeb data set image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eff89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX 3500 Ada Generation Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory C:\\Users\\maq25jh\\Documents\\Code\\counterfactual-benchmarks\\counterfactual-benchmark\\counterfactual_benchmark\\methods\\deepscm\\checkpoints\\celeba\\simple\\trained_scm exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | encoder       | Encoder       | 6.8 M  | train\n",
      "1 | decoder       | Decoder       | 13.9 M | train\n",
      "2 | discriminator | Discriminator | 18.6 M | train\n",
      "--------------------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "32.5 M    Non-trainable params\n",
      "39.3 M    Total params\n",
      "157.247   Total estimated model params size (MB)\n",
      "68        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  31%|███       | 392/1272 [26:11<58:48,  0.25it/s, v_num=2]  "
     ]
    }
   ],
   "source": [
    "for variable in causal_graph:\n",
    "    if variable not in mechanism_models: continue #only want to train variables with models, root variables don't have a causal mechanism\n",
    "    print(\"training...\")\n",
    "    train_gan(\n",
    "        gan=CelebaCondGAN(params=mechanism_models[variable][\"params\"], attr_size=config_cls[\"attribute_size\"]),\n",
    "        config=mechanism_models[variable][\"params\"],\n",
    "        data_class=Celeba,\n",
    "        graph_structure=causal_graph,\n",
    "        attribute_size=attribute_size,\n",
    "        checkpoint_dir='methods\\deepscm\\checkpoints\\celeba\\simple/trained_scm' #adjusted default path because the notebook is down one\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0618f0f",
   "metadata": {},
   "source": [
    "### Train Classifier:\n",
    "- Train the classifiers for each attribute which will be used later for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f45504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, attr, train_set, val_set, config, default_root_dir, weights=None):\n",
    "    mode = 'min' if attr in [\"age\", \"brain_vol\", \"vent_vol\", \"thickness\", \"intensity\"] else 'max'\n",
    "\n",
    "    callbacks = [\n",
    "        generate_checkpoint_callback(attr + \"_classifier\", config[\"ckpt_path\"], monitor=\"val_metric\", mode=mode),\n",
    "        generate_early_stopping_callback(patience=config[\"patience\"], monitor=\"val_metric\", mode=mode, min_delta=1e-5)\n",
    "    ]\n",
    "\n",
    "    if config[\"ema\"] == \"True\":\n",
    "        callbacks.append(generate_ema_callback(decay=0.999))\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "                      callbacks=callbacks,\n",
    "                      default_root_dir=default_root_dir, max_epochs=config[\"max_epochs\"])\n",
    "\n",
    "    if weights != None:\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(train_set), replacement=True)\n",
    "        print(\"Using sampler!\")\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_set, sampler=sampler, batch_size=config[\"batch_size_train\"],  drop_last=False, num_workers=7)\n",
    "    else:\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"batch_size_train\"], shuffle=True, drop_last=False, num_workers=7)\n",
    "\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=config[\"batch_size_val\"], shuffle=False, num_workers=7)\n",
    "    trainer.fit(classifier, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39f834b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory C:\\Users\\maq25jh\\Documents\\Code\\counterfactual-benchmarks\\counterfactual-benchmark\\methods\\deepscm\\checkpoints\\celeba\\simple\\trained_classifiers exists and is not empty.\n",
      "\n",
      "  | Name     | Type           | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | accuracy | BinaryAccuracy | 0      | train\n",
      "1 | f1_score | BinaryF1Score  | 0      | train\n",
      "2 | cnn      | Sequential     | 144 K  | train\n",
      "3 | fc       | Sequential     | 16.9 K | train\n",
      "----------------------------------------------------\n",
      "161 K     Trainable params\n",
      "0         Non-trainable params\n",
      "161 K     Total params\n",
      "0.645     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sampler!\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:428: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CelebaClassifier' object has no attribute 'attr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_root_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_cls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mckpt_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(classifier, attr, train_set, val_set, config, default_root_dir, weights)\u001b[0m\n\u001b[0;32m     21\u001b[0m     train_data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_train\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m     24\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(val_set, batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_val\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:560\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:598\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    591\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    592\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    594\u001b[0m     ckpt_path,\n\u001b[0;32m    595\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    596\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    597\u001b[0m )\n\u001b[1;32m--> 598\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1011\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m-> 1011\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1053\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1082\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1079\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    436\u001b[0m )\n\u001b[1;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\Documents\\Code\\counterfactual-benchmarks\\counterfactual-benchmark\\counterfactual_benchmark\\models\\classifiers\\celeba_classifier.py:81\u001b[0m, in \u001b[0;36mCelebaClassifier.validation_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m     79\u001b[0m     x, attrs_ \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 81\u001b[0m     y \u001b[38;5;241m=\u001b[39m attrs_[:, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattr\u001b[49m]\n\u001b[0;32m     83\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m     85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()(y_hat, y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#applies sigmoid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1963\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1964\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1966\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CelebaClassifier' object has no attribute 'attr'"
     ]
    }
   ],
   "source": [
    "for attribute in attribute_size[0].keys():\n",
    "\n",
    "    classifier = CelebaClassifier(attr=attribute, num_outputs=attribute_size[0][attribute], lr=config_cls[\"lr\"])\n",
    "\n",
    "    # train_set = Celeba(attribute_size=attribute_size, split=\"train\", transform_cls=RandomHorizontalFlip(0.5), data_dir=\"../datasets/celeba/data/img_align_celeba\")\n",
    "    train_set = Celeba(data_dir=\"datasets\\celeba\\data\", split=\"train\", transform=transforms, attribute_size=attribute_size[0], transform_cls=RandomHorizontalFlip(0.5))\n",
    "\n",
    "    val_set = Celeba(data_dir=\"datasets\\celeba\\data\", attribute_size=attribute_size[0], split=\"valid\")\n",
    "\n",
    "    #weights:\n",
    "    if attribute == \"Smiling\":\n",
    "        weights = torch.tensor(joblib.load(\"datasets\\celeba\\weights\\weights_smiling.pkl\")).double() #this path may need updating\n",
    "\n",
    "    elif attribute == \"Eyeglasses\":\n",
    "        weights = torch.tensor(joblib.load(\"datasets\\celeba\\weights\\weights_eyes.pkl\")).double()\n",
    "\n",
    "    elif attribute in {\"No_Beard\", \"Bald\"}:\n",
    "        labels = train_set.attrs[: , classifier.variables[attribute]].long()\n",
    "        print((labels == 1).sum(), (labels==0).sum())\n",
    "        class_count = torch.tensor([(labels == t).sum() for t in torch.unique(labels, sorted=True)])\n",
    "        print(class_count)\n",
    "        class_weights = 1. / class_count.float()\n",
    "\n",
    "        weights = class_weights[labels]\n",
    "        print(weights)\n",
    "\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    train_classifier(\n",
    "        classifier=classifier,\n",
    "        attr=attribute,\n",
    "        train_set=train_set,\n",
    "        val_set=val_set,\n",
    "        config=config_cls,\n",
    "        default_root_dir=config_cls[\"ckpt_path\"],\n",
    "        weights=weights\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92455e0",
   "metadata": {},
   "source": [
    "# Part 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99053ee0",
   "metadata": {},
   "source": [
    "We will start by running the model (abduction, action, and prediction), then comparing its output to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_counterfactuals(factual_batch: torch.Tensor, scm: nn.Module, do_parent:str, intervention_source: Dataset,\n",
    "                            force_change: bool = False, possible_values = None, device: str = 'cuda', bins = None):\n",
    "    factual_batch = {k: v.to(device) for k, v in factual_batch.items()}\n",
    "\n",
    "    #update with the counterfactual parent\n",
    "    if force_change:\n",
    "        possible_values = possible_values[do_parent]\n",
    "        values = factual_batch[do_parent].cpu()\n",
    "        if do_parent not in [\"digit\", \"apoE\", \"slice\"]:\n",
    "            interventions = {do_parent: torch.cat([torch.tensor(np.random.choice(possible_values[different_value(possible_values, value, bins, do_parent)])).unsqueeze(0)\n",
    "                                                for value in values]).view(-1).unsqueeze(1).to(device)}\n",
    "        else:\n",
    "            interventions = {do_parent: torch.cat([torch.tensor(rng.choice(possible_values[torch.where((different_value(possible_values, value, bins, do_parent)).any(dim=1))], axis=0)).unsqueeze(0)\n",
    "                                                for value in values]).to(device)}\n",
    "    else:\n",
    "        batch_size, _ , _ , _ = factual_batch[\"image\"].shape\n",
    "        idxs = torch.randperm(len(intervention_source))[:batch_size] # select random indices from train set to perform interventions\n",
    "\n",
    "        interventions = {do_parent: torch.cat([intervention_source[id][do_parent] for id in idxs]).view(-1).unsqueeze(1).to(device)\n",
    "                        if do_parent not in [\"digit\", \"apoE\", \"slice\"] else torch.cat([intervention_source[id][do_parent].unsqueeze(0).to(device) for id in idxs])}\n",
    "\n",
    "    abducted_noise = scm.encode(**factual_batch)\n",
    "    counterfactual_batch = scm.decode(interventions, **abducted_noise)\n",
    "\n",
    "    return counterfactual_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d433ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_effectiveness(test_set: Dataset, unnormalize_fn, batch_size:int , scm: nn.Module, attributes: List[str], do_parent:str,\n",
    "                           intervention_source: Dataset, predictors: Dict[str, Classifier], dataset: str):\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "    effectiveness_scores = {attr_key: [] for attr_key in attributes}\n",
    "    for factual_batch in tqdm(test_data_loader):\n",
    "        counterfactuals = produce_counterfactuals(factual_batch, scm, do_parent, intervention_source,\n",
    "                                                  force_change=True, possible_values=test_set.possible_values, bins=test_set.bins)\n",
    "        e_score = effectiveness(counterfactuals, unnormalize_fn, predictors, dataset)\n",
    "\n",
    "        for attr in attributes:\n",
    "            effectiveness_scores[attr].append(e_score[attr])\n",
    "\n",
    "    effectiveness_score = {key  : (round(np.mean(score), 3), round(np.std(score), 3)) for key, score in effectiveness_scores.items()}\n",
    "\n",
    "    print(f\"Effectiveness score do({do_parent}): {effectiveness_score}\")\n",
    "\n",
    "    return effectiveness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab683c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "counterfactual-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
