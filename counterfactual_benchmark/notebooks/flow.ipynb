{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d143269",
   "metadata": {},
   "source": [
    "# Counterfactual Benchmark Notebook\n",
    "- This notebook compacts what this repository does by showing an example for a GAN for Celeba Simple dataset\n",
    "- This notebook must be execute in-place inside the repo\n",
    "- Clone the repo and get a conda env with the correct packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f689e5",
   "metadata": {},
   "source": [
    "# Part 1: Training\n",
    "- Start by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700455f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from root to use local packages\n",
    "import os, sys\n",
    "os.chdir('..')\n",
    "from models.gans.celeba_gan import CelebaCondGAN\n",
    "from datasets.celeba.dataset import Celeba\n",
    "from models.classifiers.celeba_classifier import CelebaClassifier\n",
    "import torch\n",
    "import joblib\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision.transforms import RandomHorizontalFlip\n",
    "from datasets.transforms import SelectParentAttributesTransform\n",
    "from models.utils import generate_checkpoint_callback, generate_early_stopping_callback, generate_ema_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a35c15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['identity_CelebA.txt',\n",
       " 'img_align_celeba',\n",
       " 'img_align_celeba(1).zip',\n",
       " 'list_attr_celeba.txt',\n",
       " 'list_bbox_celeba.txt',\n",
       " 'list_eval_partition.txt',\n",
       " 'list_landmarks_align_celeba.txt',\n",
       " 'list_landmarks_celeba.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"datasets\\celeba\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f90664d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision.transforms import Resize, ToTensor, CenterCrop, Compose, ConvertImageDtype\n",
    "import torch\n",
    "\n",
    "transforms = Compose([CenterCrop(150), Resize((64, 64)), ToTensor(), ConvertImageDtype(dtype=torch.float32),])\n",
    "\n",
    "data = CelebA(root=\"datasets\\celeba\\data\", split=\"train\", target_type=\"attr\", transform=transforms, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc6bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_class, attribute_size, config, transform=None, **kwargs):\n",
    "    data = data_class(attribute_size=attribute_size, transform=transform, split='train', **kwargs)\n",
    "\n",
    "    if data.has_valid_set:\n",
    "        train_set = data\n",
    "        val_set = data_class(attribute_size=attribute_size, transform=transform, split='valid', **kwargs)\n",
    "    else:\n",
    "        train_set, val_set = torch.utils.data.random_split(data, [config[\"train_val_split\"], 1 - config[\"train_val_split\"]])\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"batch_size_train\"], shuffle=True, num_workers=7)\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=config[\"batch_size_val\"], shuffle=False, num_workers=7)\n",
    "    return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1099e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, config, data_class, graph_structure, attribute_size, checkpoint_dir, **kwargs):\n",
    "    transform = SelectParentAttributesTransform(\"image\", attribute_size, graph_structure)\n",
    "\n",
    "    train_data_loader, val_data_loader = get_dataloaders(data_class, attribute_size, config, transform, **kwargs)\n",
    "\n",
    "    monitor= \"fid\" if config['finetune'] == 0 else \"lpips\"\n",
    "    callbacks = [\n",
    "        generate_checkpoint_callback(gan.name, checkpoint_dir, monitor=monitor),\n",
    "        generate_early_stopping_callback(patience=config[\"patience\"], monitor=monitor)\n",
    "    ]\n",
    "\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "                      callbacks=callbacks,\n",
    "                      default_root_dir=checkpoint_dir, max_epochs=config[\"max_epochs\"])\n",
    "\n",
    "    trainer.fit(gan, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e352d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define causal graph (canibalised from config/../gan.json)\n",
    "causal_graph = {\n",
    "        \"Smiling\": [],\n",
    "        \"Eyeglasses\": [],\n",
    "        \"image\": [\"Smiling\", \"Eyeglasses\"]\n",
    "    },\n",
    "\n",
    "#define the models for each mechanism (only one for image here as the rest in the graph are roots)\n",
    "mechanism_models =  {\n",
    "        \"image\": {\n",
    "            \"model_type\": \"gan\",\n",
    "            \"model_class\": \"CelebaCondGAN\",\n",
    "            \"module\": \"models.gans\",\n",
    "            \"params\": {\n",
    "                \"n_chan_enc\": [3, 64, 128, 256, 256, 512, 512],\n",
    "                \"n_chan_gen\": [512 ,512, 256, 256, 128, 64, 3],\n",
    "                \"latent_dim\": 512,\n",
    "                \"num_continuous\": 2,\n",
    "                \"d_updates_per_g_update\": 1,\n",
    "                \"gradient_clip_val\": 0.5,\n",
    "                \"finetune\": 1,\n",
    "                \"pretrained_path\": \"\",\n",
    "                \"lr\": 1e-4,\n",
    "                \"batch_size_train\": 128,\n",
    "                \"batch_size_val\": 128,\n",
    "                \"patience\": 10,\n",
    "                \"max_epochs\": 1000\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "attribute_size = {\n",
    "        \"Smiling\": 1,\n",
    "        \"Eyeglasses\": 1\n",
    "         },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100fee9",
   "metadata": {},
   "source": [
    "### Train model:\n",
    "- Train the GAN model for Celeb data set image generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8eff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in causal_graph:\n",
    "    if variable not in mechanism_models: continue #only want to train variables with models, root variables don't have a causal mechanism\n",
    "    train_gan(\n",
    "        gan=CelebaCondGAN,\n",
    "        config=mechanism_models[variable][\"params\"],\n",
    "        data_class=Celeba,\n",
    "        graph_structure=causal_graph,\n",
    "        attribute_size=attribute_size,\n",
    "        checkpoint_dir=\"../methods/deepscm/checkpoints/celeba/simple/trained_scm\" #adjusted default path because the notebook is down one\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0618f0f",
   "metadata": {},
   "source": [
    "### Train Classifier:\n",
    "- Train the classifiers for each attribute which will be used later for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f45504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, attr, train_set, val_set, config, default_root_dir, weights=None):\n",
    "    mode = 'min' if attr in [\"age\", \"brain_vol\", \"vent_vol\", \"thickness\", \"intensity\"] else 'max'\n",
    "\n",
    "    callbacks = [\n",
    "        generate_checkpoint_callback(attr + \"_classifier\", config[\"ckpt_path\"], monitor=\"val_metric\", mode=mode),\n",
    "        generate_early_stopping_callback(patience=config[\"patience\"], monitor=\"val_metric\", mode=mode, min_delta=1e-5)\n",
    "    ]\n",
    "\n",
    "    if config[\"ema\"] == \"True\":\n",
    "        callbacks.append(generate_ema_callback(decay=0.999))\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "                      callbacks=callbacks,\n",
    "                      default_root_dir=default_root_dir, max_epochs=config[\"max_epochs\"])\n",
    "\n",
    "    if weights != None:\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(train_set), replacement=True)\n",
    "        print(\"Using sampler!\")\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_set, sampler=sampler, batch_size=config[\"batch_size_train\"],  drop_last=False, num_workers=7)\n",
    "    else:\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=config[\"batch_size_train\"], shuffle=True, drop_last=False, num_workers=7)\n",
    "\n",
    "\n",
    "    val_data_loader = torch.utils.data.DataLoader(val_set, batch_size=config[\"batch_size_val\"], shuffle=False, num_workers=7)\n",
    "    trainer.fit(classifier, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18702da",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cls = {\n",
    "    \"attribute_size\": {\n",
    "        \"Smiling\": 1,\n",
    "        \"Eyeglasses\": 1\n",
    "    },\n",
    "\n",
    "    \"dataset\": \"celeba\",\n",
    "    \"ckpt_path\" : \"../methods/deepscm/checkpoints/celeba/simple/trained_classifiers\", #modified this line for the notebook\n",
    "    \"in_shape\" : [3, 64, 64] ,\n",
    "    \"patience\" : 10,\n",
    "    \"batch_size_train\" : 128,\n",
    "    \"batch_size_val\" : 128,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"max_epochs\" : 1000,\n",
    "    \"ema\": \"True\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f834b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m attribute_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      3\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m CelebaClassifier(attr\u001b[38;5;241m=\u001b[39mattribute, num_outputs\u001b[38;5;241m=\u001b[39mattribute_size[\u001b[38;5;241m0\u001b[39m][attribute], lr\u001b[38;5;241m=\u001b[39mconfig_cls[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m     train_set \u001b[38;5;241m=\u001b[39m \u001b[43mCeleba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomHorizontalFlip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../datasets/celeba/data/img_align_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#weights:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attribute \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSmiling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\Documents\\Code\\counterfactual-benchmarks\\counterfactual-benchmark\\counterfactual_benchmark\\datasets\\celeba\\dataset.py:27\u001b[0m, in \u001b[0;36mCeleba.__init__\u001b[1;34m(self, attribute_size, split, normalize_, transform, transform_cls, data_dir)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_cls \u001b[38;5;241m=\u001b[39m transform_cls\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m attribute_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mattr_names\u001b[38;5;241m.\u001b[39mindex(attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attribute_size\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m {attr: torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mattr[:, attr_id], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m attr, attr_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(attribute_size\u001b[38;5;241m.\u001b[39mkeys(), attribute_ids)}\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\Documents\\Code\\counterfactual-benchmarks\\counterfactual-benchmark\\counterfactual_benchmark\\datasets\\celeba\\dataset.py:12\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_dir, split)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(data_dir, split):\n\u001b[0;32m     11\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m Compose([CenterCrop(\u001b[38;5;241m150\u001b[39m), Resize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)), ToTensor(), ConvertImageDtype(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),])\n\u001b[1;32m---> 12\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\maq25jh\\miniconda3\\envs\\counterfactual-benchmarks\\lib\\site-packages\\torchvision\\datasets\\celeba.py:88\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[1;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m split_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m }\n\u001b[0;32m     96\u001b[0m split_ \u001b[38;5;241m=\u001b[39m split_map[\n\u001b[0;32m     97\u001b[0m     verify_str_arg(\n\u001b[0;32m     98\u001b[0m         split\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m ]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "for attribute in attribute_size[0].keys():\n",
    "\n",
    "    classifier = CelebaClassifier(attr=attribute, num_outputs=attribute_size[0][attribute], lr=config_cls[\"lr\"])\n",
    "\n",
    "    train_set = Celeba(attribute_size=attribute_size, split=\"train\", transform_cls=RandomHorizontalFlip(0.5), data_dir=\"../datasets/celeba/data/img_align_celeba\")\n",
    "\n",
    "    #weights:\n",
    "    if attribute == \"Smiling\":\n",
    "        weights = torch.tensor(joblib.load(\"../../datasets/celeba/weights/weights_smiling.pkl\")).double() #this path may need updating\n",
    "\n",
    "    elif attribute == \"Eyeglasses\":\n",
    "        weights = torch.tensor(joblib.load(\"../../datasets/celeba/weights/weights_eyes.pkl\")).double()\n",
    "\n",
    "    elif attribute in {\"No_Beard\", \"Bald\"}:\n",
    "        labels = train_set.attrs[: , classifier.variables[attribute]].long()\n",
    "        print((labels == 1).sum(), (labels==0).sum())\n",
    "        class_count = torch.tensor([(labels == t).sum() for t in torch.unique(labels, sorted=True)])\n",
    "        print(class_count)\n",
    "        class_weights = 1. / class_count.float()\n",
    "\n",
    "        weights = class_weights[labels]\n",
    "        print(weights)\n",
    "\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    train_classifier(\n",
    "        classifier=classifier,\n",
    "        attr=attribute,\n",
    "        train_set=train_set,\n",
    "        config=config_cls,\n",
    "        default_root_dir=config_cls[\"ckpt_path\"],\n",
    "        weights=weights\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92455e0",
   "metadata": {},
   "source": [
    "# Part 2: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99053ee0",
   "metadata": {},
   "source": [
    "We will start by running the model (abduction, action, and prediction), then comparing its output to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_counterfactuals(factual_batch: torch.Tensor, scm: nn.Module, do_parent:str, intervention_source: Dataset,\n",
    "                            force_change: bool = False, possible_values = None, device: str = 'cuda', bins = None):\n",
    "    factual_batch = {k: v.to(device) for k, v in factual_batch.items()}\n",
    "\n",
    "    #update with the counterfactual parent\n",
    "    if force_change:\n",
    "        possible_values = possible_values[do_parent]\n",
    "        values = factual_batch[do_parent].cpu()\n",
    "        if do_parent not in [\"digit\", \"apoE\", \"slice\"]:\n",
    "            interventions = {do_parent: torch.cat([torch.tensor(np.random.choice(possible_values[different_value(possible_values, value, bins, do_parent)])).unsqueeze(0)\n",
    "                                                for value in values]).view(-1).unsqueeze(1).to(device)}\n",
    "        else:\n",
    "            interventions = {do_parent: torch.cat([torch.tensor(rng.choice(possible_values[torch.where((different_value(possible_values, value, bins, do_parent)).any(dim=1))], axis=0)).unsqueeze(0)\n",
    "                                                for value in values]).to(device)}\n",
    "    else:\n",
    "        batch_size, _ , _ , _ = factual_batch[\"image\"].shape\n",
    "        idxs = torch.randperm(len(intervention_source))[:batch_size] # select random indices from train set to perform interventions\n",
    "\n",
    "        interventions = {do_parent: torch.cat([intervention_source[id][do_parent] for id in idxs]).view(-1).unsqueeze(1).to(device)\n",
    "                        if do_parent not in [\"digit\", \"apoE\", \"slice\"] else torch.cat([intervention_source[id][do_parent].unsqueeze(0).to(device) for id in idxs])}\n",
    "\n",
    "    abducted_noise = scm.encode(**factual_batch)\n",
    "    counterfactual_batch = scm.decode(interventions, **abducted_noise)\n",
    "\n",
    "    return counterfactual_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d433ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_effectiveness(test_set: Dataset, unnormalize_fn, batch_size:int , scm: nn.Module, attributes: List[str], do_parent:str,\n",
    "                           intervention_source: Dataset, predictors: Dict[str, Classifier], dataset: str):\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=7)\n",
    "\n",
    "    effectiveness_scores = {attr_key: [] for attr_key in attributes}\n",
    "    for factual_batch in tqdm(test_data_loader):\n",
    "        counterfactuals = produce_counterfactuals(factual_batch, scm, do_parent, intervention_source,\n",
    "                                                  force_change=True, possible_values=test_set.possible_values, bins=test_set.bins)\n",
    "        e_score = effectiveness(counterfactuals, unnormalize_fn, predictors, dataset)\n",
    "\n",
    "        for attr in attributes:\n",
    "            effectiveness_scores[attr].append(e_score[attr])\n",
    "\n",
    "    effectiveness_score = {key  : (round(np.mean(score), 3), round(np.std(score), 3)) for key, score in effectiveness_scores.items()}\n",
    "\n",
    "    print(f\"Effectiveness score do({do_parent}): {effectiveness_score}\")\n",
    "\n",
    "    return effectiveness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab683c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "counterfactual-benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
